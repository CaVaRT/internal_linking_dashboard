{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867e8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()   \n",
    "#obtaining a session token (valid for 30 days)\n",
    "import requests\n",
    "\n",
    "# secret variables\n",
    "LUMAR_SECRET = os.getenv(\"LUMAR_SECRET\")\n",
    "LUMAR_USER_KEY_ID = os.getenv(\"LUMAR_USER_KEY_ID\")\n",
    "LUMAR_CRAWL_ID = os.getenv(\"LUMAR_CRAWL_ID\")\n",
    "\n",
    "if not LUMAR_SECRET:\n",
    "    raise RuntimeError(\"Missing LUMAR_SECRET. Put it in a .env file or environment\")\n",
    "\n",
    "# GraphQL endpoint for Lumar API\n",
    "url = 'https://api.lumar.io/graphql'\n",
    "#GraphQL mutation (message)\n",
    "query = \"\"\"\n",
    "mutation LoginWithUserKey($secret: String!, $userKeyId: ObjectID!) {\n",
    "  createSessionUsingUserKey(input: { userKeyId: $userKeyId, secret: $secret }) {\n",
    "    token\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "#variables for the mutation\n",
    "variables = {\n",
    "    \"secret\": LUMAR_SECRET,\n",
    "    \"userKeyId\": LUMAR_USER_KEY_ID\n",
    "}\n",
    "#send the request\n",
    "response = requests.post(url, json={'query':query, \"variables\":variables})\n",
    "#Get the response JSON\n",
    "data = response.json()\n",
    "#Extract the session token\n",
    "token = data['data']['createSessionUsingUserKey']['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "class LumarAPIClient:\n",
    "    def __init__(self, api_token):\n",
    "        self.api_token = api_token\n",
    "        self.base_url = \"https://api.lumar.io/graphql\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"apollographql-client-name\": \"python-client\",\n",
    "            \"apollographql-client-version\": \"1.0.0\",\n",
    "            \"x-auth-token\": api_token\n",
    "        }\n",
    "    \n",
    "    def fetch_unique_internal_links(self, crawl_id, report_template_code=\"unique_internal_links\"):\n",
    "        \"\"\"\n",
    "        Fetch all unique internal links data with pagination\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        after_cursor = None\n",
    "        has_next_page = True\n",
    "        \n",
    "        query = \"\"\"\n",
    "        query GetReportStatForCrawl(\n",
    "            $crawlId: ObjectID!\n",
    "            $reportTemplateCode: String!\n",
    "            $after: String\n",
    "        ) {\n",
    "            getReportStat(\n",
    "                input: {crawlId: $crawlId, reportTemplateCode: $reportTemplateCode}\n",
    "            ) {\n",
    "                crawlUniqueLinks(after: $after, reportType: Basic) {\n",
    "                    nodes {\n",
    "                        urlTo\n",
    "                        primaryUrlFrom\n",
    "                        anchorText\n",
    "                        instanceCount\n",
    "                    }\n",
    "                    pageInfo {\n",
    "                        endCursor\n",
    "                        hasNextPage\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        while has_next_page:\n",
    "            variables = {\n",
    "                \"crawlId\": crawl_id,\n",
    "                \"reportTemplateCode\": report_template_code\n",
    "            }\n",
    "            if after_cursor:\n",
    "                variables[\"after\"] = after_cursor\n",
    "            \n",
    "            response = requests.post(\n",
    "                self.base_url,\n",
    "                headers=self.headers,\n",
    "                json={\"query\": query, \"variables\": variables}\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "            \n",
    "            data = response.json()\n",
    "            if 'errors' in data:\n",
    "                raise Exception(f\"GraphQL errors: {data['errors']}\")\n",
    "            \n",
    "            crawl_data = data['data']['getReportStat']['crawlUniqueLinks']\n",
    "            all_data.extend(crawl_data['nodes'])\n",
    "            \n",
    "            page_info = crawl_data['pageInfo']\n",
    "            has_next_page = page_info['hasNextPage']\n",
    "            after_cursor = page_info['endCursor']\n",
    "            \n",
    "            print(f\"Fetched {len(crawl_data['nodes'])} records. Total so far: {len(all_data)}\")\n",
    "        \n",
    "        print(f\"Completed! Total records fetched: {len(all_data)}\")\n",
    "        return all_data\n",
    "    \n",
    "    def to_dataframe(self, raw_data):\n",
    "        \"\"\"\n",
    "        Convert raw API data into a DataFrame with aggregated anchor text counts.\n",
    "        Returns a DataFrame with columns:\n",
    "          - target_url\n",
    "          - anchor_texts: aggregated like 'anchor1 (3) | anchor2 (1)' (counts are summed instanceCount)\n",
    "          - unique_anchor_text_count: number of distinct anchorText values\n",
    "          - total_inlinks: sum of instanceCount for the target_url\n",
    "          - found_at: semicolon-separated unique primaryUrlFrom values\n",
    "        \"\"\"\n",
    "        # handle empty input\n",
    "        if not raw_data:\n",
    "            return pd.DataFrame(columns=['target_url','anchor_texts','unique_anchor_text_count','total_inlinks','found_at'])\n",
    "        df = pd.DataFrame(raw_data)\n",
    "        df.rename(columns={\"urlTo\": \"target_url\", \"primaryUrlFrom\": \"found_at\"}, inplace=True)\n",
    "        # ensure instanceCount exists and is integer\n",
    "        if 'instanceCount' not in df.columns:\n",
    "            df['instanceCount'] = 1\n",
    "        df['instanceCount'] = pd.to_numeric(df['instanceCount'], errors='coerce').fillna(0).astype(int)\n",
    "        # Group by target_url, anchorText, and found_at to sum instance counts per anchor\n",
    "        agg = df.groupby(['target_url','anchorText','found_at'], dropna=False)['instanceCount'].sum().reset_index()\n",
    "        # Build aggregated anchorText strings per target_url\n",
    "        def _anchor_string(sub):\n",
    "            parts = []\n",
    "            for _, r in sub.iterrows():\n",
    "                anchor = r['anchorText'] if pd.notna(r['anchorText']) else ''\n",
    "                parts.append(f\"{anchor} ({r['instanceCount']})\".strip())\n",
    "            return ' | '.join([p for p in parts if p])\n",
    "        anchor_strings = agg.groupby('target_url').apply(_anchor_string).rename('anchor_texts')\n",
    "        # Compute unique anchor text counts and total inlinks per target_url\n",
    "        grouped = agg.groupby('target_url').agg({\n",
    "            'anchorText': lambda s: s.nunique(),\n",
    "            'instanceCount': 'sum',\n",
    "            'found_at': lambda s: '; '.join(sorted(set([str(x) for x in s if pd.notna(x)])))\n",
    "        }).rename(columns={'anchorText': 'unique_anchor_text_count', 'instanceCount': 'total_inlinks'})\n",
    "        result = grouped.join(anchor_strings).reset_index()\n",
    "        # Ensure column order\n",
    "        result = result[['target_url','anchor_texts','unique_anchor_text_count','total_inlinks','found_at']]\n",
    "        return result\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    API_TOKEN = token  # replace with your token\n",
    "    CRAWL_ID = LUMAR_CRAWL_ID\n",
    "    \n",
    "    client = LumarAPIClient(API_TOKEN)\n",
    "    \n",
    "    try:\n",
    "        print(\"Fetching data...\")\n",
    "        raw_data = client.fetch_unique_internal_links(CRAWL_ID)\n",
    "        \n",
    "        print(\"Converting to DataFrame...\")\n",
    "       \n",
    "        df = client.to_dataframe(raw_data)\n",
    "        \n",
    "        print(f\"\\nDataFrame created with {len(df)} rows\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(\"lumar_internal_links.csv\", index=False)\n",
    "        print(\"\\nSaved to lumar_internal_links.csv\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
